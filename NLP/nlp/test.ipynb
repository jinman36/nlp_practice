{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/copper_turtle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/copper_turtle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/copper_turtle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_85215/2047234332.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub(\"^RT\\s+\", default_replace, tweet)\n",
      "/tmp/ipykernel_85215/2047234332.py:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  tweet = re.sub(\"\\B@\\w+\", default_replace, tweet)\n",
      "/tmp/ipykernel_85215/2047234332.py:10: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def replace_retweet(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub(\"^RT\\s+\", default_replace, tweet)\n",
    "  return tweet\n",
    "\n",
    "def replace_user(tweet, default_replace=\"twitteruser\"):\n",
    "  tweet = re.sub(\"\\B@\\w+\", default_replace, tweet)\n",
    "  return tweet\n",
    "\n",
    "def replace_url(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
    "  return tweet\n",
    "\n",
    "def replace_hashtag(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub('#+', default_replace, tweet)\n",
    "  return tweet\n",
    "\n",
    "def to_lowercase(tweet):\n",
    "  return tweet.lower()\n",
    "\n",
    "def word_repetition(tweet):\n",
    "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "  return tweet\n",
    "\n",
    "def punct_repetition(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.!])', default_replace, tweet)\n",
    "  return tweet\n",
    "\n",
    "def fix_contractions(tweet):\n",
    "  return contractions.fix(tweet)\n",
    "\n",
    "def tokenize(tweet):\n",
    "  tokens = word_tokenize(tweet)\n",
    "  return tokens\n",
    "\n",
    "def demojize(tweet):\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(tweet, keep_punct = False, keep_alnum = False, keep_stop = False):\n",
    "    token_list = word_tokenize(tweet)\n",
    "    \n",
    "    if not keep_punct:\n",
    "        token_list = [token for token in token_list if token not in string.punctuation]\n",
    "        \n",
    "    if not keep_alnum:\n",
    "        token_list = [token for token in token_list if token.isalpha()]\n",
    "    \n",
    "    if not keep_stop:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.discard('not')\n",
    "        token_list = [token for token in token_list if not token in stop_words]\n",
    "        \n",
    "          \n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        token_list.append(stemmer.stem(token))\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, verbose=False):\n",
    "  # if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
    "  # print(tweet)\n",
    "\n",
    "\n",
    "\n",
    "  ## Twitter Features\n",
    "    # replace retweet\n",
    "  tweet = replace_retweet(tweet, default_replace=\"\")\n",
    "  # replace user tag\n",
    "  tweet = replace_user(tweet, default_replace=\"twitteruser\")\n",
    "  # replace url\n",
    "  tweet = replace_url(tweet, default_replace=\"\")\n",
    "  # replace hashtag\n",
    "  tweet = replace_hashtag(tweet, default_replace=\"\")\n",
    "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
    "\n",
    "  ## Word Features\n",
    "    # lower case\n",
    "  tweet = to_lowercase(tweet)\n",
    "    # replace contractions\n",
    "  tweet = fix_contractions(tweet)\n",
    "    # replace punctuation repetition\n",
    "  tweet = punct_repetition(tweet, default_replace=\"\")\n",
    "    # replace word repetition\n",
    "  tweet = word_repetition(tweet)\n",
    "    # replace emojis\n",
    "  tweet = demojize(tweet)\n",
    "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
    "\n",
    "  ## Tokenization & Stemming\n",
    "    # tokenize\n",
    "  tokens = custom_tokenize(tweet, keep_alnum=True)\n",
    "    # define stemmer\n",
    "    # stem tokens\n",
    "  stem = stem_tokens(tokens, SnowballStemmer(\"english\"))\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'wasn', 'couldn', \"should've\", 'while', 'you', 'that', \"couldn't\", 'ours', 'who', 'on', 'theirs', 'having', \"don't\", \"shouldn't\", 'they', 'then', 'herself', 'just', \"doesn't\", 'before', 'isn', \"isn't\", 'haven', 'myself', 'itself', 'his', 'because', 'same', 'their', 'once', 'we', \"you've\", 'o', \"mightn't\", 'what', 'shouldn', 'themselves', \"you'll\", \"won't\", 'has', 'below', 'to', 'me', 'during', 'own', 'are', 'off', 'more', 'y', 'here', 'out', 'ain', 'hers', 'weren', 'i', 'be', 'about', 'he', 'through', 'nor', 'too', \"didn't\", 'didn', 'which', 'down', 'yours', 'up', 'both', 'after', 'there', 'with', 'these', 'any', 'but', 're', 'doesn', \"needn't\", 'been', 'being', 'hasn', 'will', 'only', 'against', 'most', 'over', 'won', 'this', 'why', 'very', 'of', 'under', 'himself', 'have', 'can', 'such', 'my', 've', 'again', 'between', 'hadn', 'should', \"you'd\", 'from', 'yourself', \"mustn't\", 'in', 'ourselves', 'how', 'him', 'don', \"hadn't\", 't', 'each', \"haven't\", 'mustn', 'yourselves', 'where', \"she's\", 'its', 'was', 'does', 'had', 'by', 'further', 'some', 'needn', 'were', 'shan', \"wouldn't\", 'at', \"shan't\", 'into', 'no', 'whom', 'do', 'when', 's', \"wasn't\", 'those', 'she', \"it's\", 'them', 'other', 'for', 'your', 'doing', 'or', 'll', 'all', 'as', 'so', 'few', 'if', \"hasn't\", \"that'll\", 'ma', 'her', 'is', 'it', 'now', 'did', 'than', 'not', 'mightn', 'until', \"you're\", 'am', 'd', \"aren't\", 'a', 'm', 'wouldn', 'our', 'and', 'above', \"weren't\", 'an', 'aren'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"tweet_text\"].apply(process_tweet)\n",
    "df[\"tweet_sentiment\"] = df[\"sentiment\"].apply(lambda i: 1\n",
    "                                              if i == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[layin, n, bed, headach, ughh.waitin, call]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[funer, ceremony.gloomi, friday]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[re-ping, twitterus, not, go, prom, bf, not, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>negative</td>\n",
       "      <td>[hmm]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18722</th>\n",
       "      <td>1753918818</td>\n",
       "      <td>had SUCH and AMAZING time last night, McFly we...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[amaz, time, last, night, mcfli, incred]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18723</th>\n",
       "      <td>1753918881</td>\n",
       "      <td>@jasimmo Ooo showing of your French skills!! l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[twitterus, oo, show, french, skill, lol, thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18724</th>\n",
       "      <td>1753918900</td>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[succes, follow, tayla]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18725</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>positive</td>\n",
       "      <td>[happi, mother, day, love]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18726</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[happi, mother, 's, day, mommi, woman, man, lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18727 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                         tweet_text  \\\n",
       "0      1956967666  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "1      1956967696                Funeral ceremony...gloomy friday...   \n",
       "2      1956967789               wants to hang out with friends SOON!   \n",
       "3      1956968477  Re-pinging @ghostridah14: why didn't you go to...   \n",
       "4      1956968636               Hmmm. http://www.djhero.com/ is down   \n",
       "...           ...                                                ...   \n",
       "18722  1753918818  had SUCH and AMAZING time last night, McFly we...   \n",
       "18723  1753918881  @jasimmo Ooo showing of your French skills!! l...   \n",
       "18724  1753918900                      Succesfully following Tayla!!   \n",
       "18725  1753919001                     Happy Mothers Day  All my love   \n",
       "18726  1753919005  Happy Mother's Day to all the mommies out ther...   \n",
       "\n",
       "      sentiment                                             tokens  \\\n",
       "0      negative        [layin, n, bed, headach, ughh.waitin, call]   \n",
       "1      negative                   [funer, ceremony.gloomi, friday]   \n",
       "2      positive                         [want, hang, friend, soon]   \n",
       "3      negative  [re-ping, twitterus, not, go, prom, bf, not, l...   \n",
       "4      negative                                              [hmm]   \n",
       "...         ...                                                ...   \n",
       "18722  positive           [amaz, time, last, night, mcfli, incred]   \n",
       "18723  positive  [twitterus, oo, show, french, skill, lol, thin...   \n",
       "18724  positive                            [succes, follow, tayla]   \n",
       "18725  positive                         [happi, mother, day, love]   \n",
       "18726  positive  [happi, mother, 's, day, mommi, woman, man, lo...   \n",
       "\n",
       "       tweet_sentiment  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    1  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "18722                1  \n",
       "18723                1  \n",
       "18724                1  \n",
       "18725                1  \n",
       "18726                1  \n",
       "\n",
       "[18727 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"tokens\"]\n",
    "y = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (12547,)\n",
      "Testing Data Shape:  (6180,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# count_vect.fit(X_train)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# X_train_counts = count_vect.transform(X_train)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcount_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_ml/lib/python3.12/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_ml/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_ml/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_ml/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_ml/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# count_vect.fit(X_train)\n",
    "# X_train_counts = count_vect.transform(X_train)\n",
    "\n",
    "count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
